# -*- coding: utf-8 -*-
"""housepriceprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oygNZSl5-47lp_wLitT2JYr5gjoDn7wJ
"""

import pandas as pd

try:
    df = pd.read_csv('train.csv')
    print(df.shape)
    display(df.head())
except FileNotFoundError:
    print("Error: 'train.csv' not found.")
    df = None  # Set df to None to indicate failure
except pd.errors.ParserError:
    print("Error: Could not parse 'train.csv'. Check file format.")
    df = None
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    df = None

# Examine the shape of the DataFrame.
print("Shape of the DataFrame:", df.shape)

# Display the first few rows of the DataFrame.
display(df.head())

# Get a summary of the DataFrame's structure.
display(df.info())

# Calculate descriptive statistics for numerical features.
display(df.describe())

# Identify and count missing values in each column.
missing_values = df.isnull().sum()
print("\nMissing Values per column:\n", missing_values)

# Analyze the distribution of numerical features using histograms.
import matplotlib.pyplot as plt
df.hist(figsize=(20,20), bins=50)
plt.tight_layout()
plt.show()

"""**Reasoning**:
Analyze the distribution of categorical features using value counts.


"""

# Analyze the distribution of categorical features.
for column in df.select_dtypes(include=['object']).columns:
    print(f"\nValue counts for '{column}':")
    display(df[column].value_counts())

"""## Data cleaning

### Subtask:
Clean the data by handling missing values and inconsistencies.

**Reasoning**:
Impute missing values for numerical features using appropriate methods, handle inconsistent data types, and remove duplicate rows.
"""

# Impute missing values for numerical features
for col in ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']:
    if df[col].isnull().any():
        if col == 'LotFrontage':
            df[col] = df[col].fillna(df[col].median())  # Median is less sensitive to outliers
        elif col == 'MasVnrArea':
            df[col] = df[col].fillna(0)  # 0 is a reasonable assumption for missing veneer area
        elif col == 'GarageYrBlt':
            df[col] = df[col].fillna(df['YearBuilt']) #Use yearbuilt as imputation for garage year built

# Check for inconsistent data types and correct them if necessary.
# (No inconsistencies found during data exploration, so no action needed here)

# Remove duplicate rows
df.drop_duplicates(inplace=True)

"""## Data preparation

### Subtask:
Prepare the data for modeling by encoding categorical features and scaling numerical features.

**Reasoning**:
Identify categorical and numerical columns, apply one-hot encoding to categorical features, and scale numerical features using StandardScaler.  Then, concatenate the processed features.
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Identify categorical columns
categorical_cols = df.select_dtypes(include='object').columns

# One-hot encode categorical features
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Identify numerical columns (excluding 'SalePrice')
numerical_cols = df.select_dtypes(exclude='object').columns
numerical_cols = numerical_cols.drop('SalePrice')

# Scale numerical features
scaler = StandardScaler()
df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])

# The final DataFrame is ready for model training
display(df_encoded.head())

"""## Data splitting

### Subtask:
Split the preprocessed data into training and testing sets.

**Reasoning**:
Split the preprocessed data (df_encoded) into training and testing sets using train_test_split.
"""

from sklearn.model_selection import train_test_split

# Assuming 'SalePrice' is the target variable
X = df_encoded.drop('SalePrice', axis=1)
y = df_encoded['SalePrice']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of the resulting DataFrames to verify the split
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""## Model training

### Subtask:
Train a RandomForestRegressor model on the prepared training data.

**Reasoning**:
Train a RandomForestRegressor model using the prepared training data (X_train, y_train).
"""

from sklearn.ensemble import RandomForestRegressor

# Initialize the model with hyperparameters
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
try:
    model.fit(X_train, y_train)
    print("Model trained successfully.")
except ValueError as e:
    print(f"Error during model training: {e}")
except Exception as e:
    print(f"An unexpected error occurred during model training: {e}")

"""## Model evaluation

### Subtask:
Evaluate the trained RandomForestRegressor model's performance on the test set.

**Reasoning**:
Use the trained model to predict on the test set and evaluate the model's performance using MAE, MSE, RMSE, and R-squared.
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Predict on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R2: {r2:.4f}")

"""## Summary:

### 1. Q&A

Based on the provided data analysis, the model achieved the following performance metrics on the test set:

* **MAE (Mean Absolute Error):** 17661.1885
* **MSE (Mean Squared Error):** 842980412.5723
* **RMSE (Root Mean Squared Error):** 29034.1250
* **R2 (R-squared):** 0.8901

The R-squared value of 0.89 indicates that the model explains 89% of the variance in the target variable ('SalePrice'), suggesting a relatively good fit.

### 2. Data Analysis Key Findings

* **Missing Value Imputation:** Missing values in `LotFrontage`, `MasVnrArea`, and `GarageYrBlt` were imputed using the median, 0, and `YearBuilt`, respectively.
* **Data Encoding and Scaling:**  Categorical features were one-hot encoded, resulting in 246 columns in the final DataFrame (`df_encoded`). Numerical features were scaled using `StandardScaler`.
* **Model Performance:** The trained `RandomForestRegressor` achieved an R-squared score of 0.8901 on the test set, indicating a strong fit.  The RMSE was 29034.1250.

### 3. Insights or Next Steps

* **Feature Engineering:** Explore potential feature engineering opportunities to improve model performance.  For example, creating interaction terms or polynomial features.
* **Hyperparameter Tuning:** Perform hyperparameter tuning on the `RandomForestRegressor` model (e.g., using GridSearchCV or RandomizedSearchCV) to potentially optimize its performance.

"""